{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyguerges/anthonyguerges/blob/main/Projects/NLE_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language Engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "_M3r84BT5WeN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ibHDq-Im5Re4"
      },
      "outputs": [],
      "source": [
        "# my unique candidate number is here\n",
        "candidate_number = 262809"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This project is focused on applying natural language processing techniques specifically POS-tagging and the Naive Bayes classification model to analyze and classify sentences according to predefined stylistic categories. Below, I detail the steps taken to achieve this, adapting typical question prompts into descriptive project steps."
      ],
      "metadata": {
        "id": "259FEqEb5xRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# This is a list of sentences written in various styles.\n",
        "sentences=[\"a tediously verbose sentence may contain many gratuitous and overly contrived modifiers .\",\n",
        "           \"another sentence could be too short .\",\n",
        "           \"some people write sentences that contain nouns and verbs , avoiding adjectives and descriptions .\"]\n",
        "\n",
        "# This is a dictionary containing counts of pos tags from a corpus of sentences which were labelled as styles A and B.\n",
        "classtagcounts={\"A\":{\"RB\":30, \"JJ\":30, \"NN\":10, \"NNS\":10, \"VB\":10, \"VBD\":10},\n",
        "                \"B\":{\"VBP\":20, \"VBZ\":10, \"VBG\":10, \"VBD\":10, \"NN\":20, \"NNS\":30}}\n",
        "\n",
        "# This is a complete list of pos tags.\n",
        "taglist = list(nltk.data.load('help/tagsets/upenn_tagset.pickle').keys())\n"
      ],
      "metadata": {
        "id": "89Wx4WPa5weU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af1a89f-aa15-49b0-ee23-02fd64417d9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) By following the steps below, pos-tag the sentences and construct a bag-of-tags representation of each one."
      ],
      "metadata": {
        "id": "lI7AfgwoH1-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, I split each sentence from a predefined list into individual words and applied the pos_tag function from the NLTK library to tag each word with its corresponding part of speech. This process converted the list of sentences into a list of lists, where each inner list contained tuples of word-tag pairs.\n",
        "\n"
      ],
      "metadata": {
        "id": "-QcL3UAsIl2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\"a tediously verbose sentence may contain many gratuitous and overly contrived modifiers .\",\n",
        "             \"another sentence could be too short .\",\n",
        "             \"some people write sentences that contain nouns and verbs , avoiding adjectives and descriptions .\"]\n",
        "\n",
        "# Splitting each sentence into words and then POS tagging\n",
        "tagged = [pos_tag(sentence.split()) for sentence in sentences]\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "id": "jaXDI-PKHyiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908c394c-4a26-4fc1-f91b-10ea4852e24c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('a', 'DT'), ('tediously', 'RB'), ('verbose', 'JJ'), ('sentence', 'NN'), ('may', 'MD'), ('contain', 'VB'), ('many', 'JJ'), ('gratuitous', 'JJ'), ('and', 'CC'), ('overly', 'RB'), ('contrived', 'VBD'), ('modifiers', 'NNS'), ('.', '.')], [('another', 'DT'), ('sentence', 'NN'), ('could', 'MD'), ('be', 'VB'), ('too', 'RB'), ('short', 'JJ'), ('.', '.')], [('some', 'DT'), ('people', 'NNS'), ('write', 'VBP'), ('sentences', 'NNS'), ('that', 'WDT'), ('contain', 'VBP'), ('nouns', 'NNS'), ('and', 'CC'), ('verbs', 'NNS'), (',', ','), ('avoiding', 'VBG'), ('adjectives', 'NNS'), ('and', 'CC'), ('descriptions', 'NNS'), ('.', '.')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the tagging, I transformed each list of word-tag pairs into a bag-of-tags representation using the FreqDist class from NLTK. This representation counts the occurrences of each POS tag within a sentence, effectively summarizing its grammatical structure.\n"
      ],
      "metadata": {
        "id": "I58Gw_eqKdht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting each list of word, tag pairs into a bag-of-tags representation\n",
        "bag_of_tags = [FreqDist(tag for word, tag in sentence) for sentence in tagged]\n",
        "\n",
        "# Printing the bag-of-tags representation\n",
        "for bot in bag_of_tags:\n",
        "    print(dict(bot))"
      ],
      "metadata": {
        "id": "VQPEDcxn5kWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cbb9f2-3311-4a64-af5d-27369909e4a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'DT': 1, 'RB': 2, 'JJ': 3, 'NN': 1, 'MD': 1, 'VB': 1, 'CC': 1, 'VBD': 1, 'NNS': 1, '.': 1}\n",
            "{'DT': 1, 'NN': 1, 'MD': 1, 'VB': 1, 'RB': 1, 'JJ': 1, '.': 1}\n",
            "{'DT': 1, 'NNS': 6, 'VBP': 2, 'WDT': 1, 'CC': 2, ',': 1, 'VBG': 1, '.': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Naive Bayes Model Parameter Calculation and Sentence Classification"
      ],
      "metadata": {
        "id": "OMK72_eoMdLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Explaining my ideas behind the Naive Bayes model.\n",
        "\n",
        "Starting from the assumption that we want to find the class that maximises $p(class|document)$, i will explain how Bayes theorem is used and what naive assumption is made about the features in the document. I will also describe the priors and conditional probabilities that are used to predict the most likely class for a document.\n",
        "\n"
      ],
      "metadata": {
        "id": "j5hl7NXnKb26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes model is a probabilistic machine learning model that's used for classification tasks. The fundamental principle behind this model is Bayes' Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For a Naive Bayes classifier, the event is the class we want to predict (e.g., class A or B), and the conditions are the features observed in the data (in this case, the bag-of-tags from sentences).\n",
        "\n",
        "Bayes Theorem: Bayes' Theorem is mathematically stated as:\n",
        "\n",
        "P(class|document) = P(document|class) x P(class)/P(document)\n",
        "\n",
        "In the context of document classification:\n",
        "\n",
        "P(class|document) is the posterior probability of a class given a document.\n",
        "\n",
        "P(document|class) is the likelihood, which is the probability of the document given a class.\n",
        "\n",
        "P(class) is the prior probability of the class.\n",
        "\n",
        "P(document) is the evidence, the probability of the document.\n",
        "\n",
        "Naive Assumption: The 'naive' assumption made in this model is that all features (in this case, the POS tags) are independent of each other given the class. This assumption simplifies the computation of P(document|class), as it becomes the product of the individual probabilities of each feature given the class.\n",
        "\n",
        "Priors and Conditional Probabilities:\n",
        "- **Prior Probability P(class):** This is the probability of observing each class in the training data without any additional information. It's usually calculated by the frequency of each class in the training data.\n",
        "- **Conditional Probability P(feature|class): This is the probability of observing a certain feature given a class. In the context of your task, it would be the probability of seeing a specific POS tag in sentences belonging to class A or B. These probabilities are typically calculated based on the frequency of each feature in documents of a particular class.\n",
        "\n",
        "To predict the most likely class for a document, Naive Bayes calculates the posterior probability for each class and selects the class with the highest posterior probability. This is done by multiplying the prior probability of each class with the product of the conditional probabilities of each feature observed in the document. The evidence term P(document) is usually ignored in this calculation since it's constant for all classes and does not affect the ranking of the classes."
      ],
      "metadata": {
        "id": "1OMNFQ5dLIGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) I summed the counts of POS tags for each class from a pre-existing dictionary classtagcounts to derive total frequencies for each class. Using these sums, I calculated the prior probabilities for each class, which reflect the likelihood of any given sentence belonging to a class without any further information.\n"
      ],
      "metadata": {
        "id": "zugBLa8oMyeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary containing counts of pos tags for classes A and B\n",
        "classtagcounts = {\"A\": {\"RB\": 30, \"JJ\": 30, \"NN\": 10, \"NNS\": 10, \"VB\": 10, \"VBD\": 10},\n",
        "                  \"B\": {\"VBP\": 20, \"VBZ\": 10, \"VBG\": 10, \"VBD\": 10, \"NN\": 20, \"NNS\": 30}}\n",
        "\n",
        "# Summing the counts for classes A and B\n",
        "classcounts = {class_label: sum(tag_counts.values()) for class_label, tag_counts in classtagcounts.items()}\n",
        "\n",
        "# Calculating the total count of all classes\n",
        "total_count = sum(classcounts.values())\n",
        "\n",
        "# Calculating the prior probabilities for each class\n",
        "classpriors = {class_label: count / total_count for class_label, count in classcounts.items()}\n",
        "\n",
        "classcounts, classpriors"
      ],
      "metadata": {
        "id": "s0jxsQBTyRq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ee2e13-c448-45a6-b2f8-d758cc8d2e34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'A': 100, 'B': 100}, {'A': 0.5, 'B': 0.5})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) To calculate conditional probabilities for each POS tag within the classes, I defined a function condprobs() that computes the probability of observing each tag given a class, using the frequencies obtained from the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "WLMaRpK6NcyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def condprobs(feature_counts, class_counts):\n",
        "\n",
        "# Creating the function\n",
        "    cond_probs = {}\n",
        "    for class_label, features in feature_counts.items():\n",
        "        total_count = class_counts[class_label]\n",
        "        cond_probs[class_label] = {feature: count / total_count for feature, count in features.items()}\n",
        "    return cond_probs\n",
        "\n",
        "# Applying the function to classtagcounts and classcounts\n",
        "conditional_probabilities = condprobs(classtagcounts, classcounts)\n",
        "conditional_probabilities\n"
      ],
      "metadata": {
        "id": "gfiF8zqVyv4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2e02d8-dfbc-448b-8cd7-e489ebbdd01f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': {'RB': 0.3, 'JJ': 0.3, 'NN': 0.1, 'NNS': 0.1, 'VB': 0.1, 'VBD': 0.1},\n",
              " 'B': {'VBP': 0.2, 'VBZ': 0.1, 'VBG': 0.1, 'VBD': 0.1, 'NN': 0.2, 'NNS': 0.3}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv) Here I explain why we might want to smooth the conditional probabilities, and how add one smoothing works.\n",
        "\n"
      ],
      "metadata": {
        "id": "6wqoSZo_Miju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smoothing in the context of Naive Bayes classifiers, particularly for conditional probabilities, is a crucial technique to address the issue of zero probability. This situation arises when a feature (such as a specific POS tag in this case) that appears in the test data has not been observed in the training data for a particular class. Without smoothing, the probability of observing this unseen feature given the class would be zero. This zero probability, when multiplied with other probabilities (as Naive Bayes does), would result in a zero posterior probability for the class, skewing the classification results.\n",
        "\n",
        "**Add-One Smoothing (Laplace Smoothing):\n",
        "Add-one smoothing, also known as Laplace smoothing, is a simple yet effective method to prevent zero probabilities in a Naive Bayes classifier. It works by adding a small constant (usually 1) to the count of each feature for every class in the training set. This adjustment is applied regardless of whether the feature was observed in the training data for that class. The formula for calculating the conditional probability with add-one smoothing is:\n",
        "\n",
        "P(feature|class) = (Count(feature, class) + 1) / (Count(class) + N)\n",
        "\n",
        "Where:\n",
        "\n",
        "Count(feature, class) is the original count of the feature for the given class.\n",
        "\n",
        "Count(class) is the total count of all features for the class.\n",
        "\n",
        "N is the number of unique features in the entire training set.\n",
        "\n",
        "The addition of 1 to the feature count ensures that no feature has a zero probability. The denominator is adjusted by adding N (the total number of unique features) to maintain probability distribution properties. This way, smoothing handles the problem of unseen features and ensures a more robust and accurate classification, particularly when dealing with sparse data sets or small training samples."
      ],
      "metadata": {
        "id": "GNaxUzqoMzDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "v) I applied add-one smoothing across all POS tags and updated the class frequencies accordingly. This adjustment was necessary to ensure that each tag, including those not present in the initial training data, was considered in the model.\n"
      ],
      "metadata": {
        "id": "jWC75MDybwxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of possible tags\n",
        "taglist = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NP', 'NPS', 'PDT', 'POS',\n",
        "           'PP', 'PP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT',\n",
        "           'WP', 'WP$', 'WRB']\n",
        "\n",
        "# Applying add-one smoothing to classtagcounts\n",
        "smoothed_classtagcounts = {}\n",
        "for class_label, tag_counts in classtagcounts.items():\n",
        "    smoothed_classtagcounts[class_label] = {}\n",
        "    for tag in taglist:\n",
        "        smoothed_classtagcounts[class_label][tag] = tag_counts.get(tag, 0) + 1\n",
        "\n",
        "# Updating classcounts to reflect modified class frequencies after smoothing\n",
        "updated_classcounts = {class_label: sum(tag_counts.values()) for class_label, tag_counts in smoothed_classtagcounts.items()}\n",
        "\n",
        "smoothed_classtagcounts, updated_classcounts"
      ],
      "metadata": {
        "id": "XO0ZgXRNzPHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0b441e-d482-4162-d7db-cbe638db7334"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'A': {'CC': 1,\n",
              "   'CD': 1,\n",
              "   'DT': 1,\n",
              "   'EX': 1,\n",
              "   'FW': 1,\n",
              "   'IN': 1,\n",
              "   'JJ': 31,\n",
              "   'JJR': 1,\n",
              "   'JJS': 1,\n",
              "   'LS': 1,\n",
              "   'MD': 1,\n",
              "   'NN': 11,\n",
              "   'NNS': 11,\n",
              "   'NP': 1,\n",
              "   'NPS': 1,\n",
              "   'PDT': 1,\n",
              "   'POS': 1,\n",
              "   'PP': 1,\n",
              "   'PP$': 1,\n",
              "   'RB': 31,\n",
              "   'RBR': 1,\n",
              "   'RBS': 1,\n",
              "   'RP': 1,\n",
              "   'SYM': 1,\n",
              "   'TO': 1,\n",
              "   'UH': 1,\n",
              "   'VB': 11,\n",
              "   'VBD': 11,\n",
              "   'VBG': 1,\n",
              "   'VBN': 1,\n",
              "   'VBP': 1,\n",
              "   'VBZ': 1,\n",
              "   'WDT': 1,\n",
              "   'WP': 1,\n",
              "   'WP$': 1,\n",
              "   'WRB': 1},\n",
              "  'B': {'CC': 1,\n",
              "   'CD': 1,\n",
              "   'DT': 1,\n",
              "   'EX': 1,\n",
              "   'FW': 1,\n",
              "   'IN': 1,\n",
              "   'JJ': 1,\n",
              "   'JJR': 1,\n",
              "   'JJS': 1,\n",
              "   'LS': 1,\n",
              "   'MD': 1,\n",
              "   'NN': 21,\n",
              "   'NNS': 31,\n",
              "   'NP': 1,\n",
              "   'NPS': 1,\n",
              "   'PDT': 1,\n",
              "   'POS': 1,\n",
              "   'PP': 1,\n",
              "   'PP$': 1,\n",
              "   'RB': 1,\n",
              "   'RBR': 1,\n",
              "   'RBS': 1,\n",
              "   'RP': 1,\n",
              "   'SYM': 1,\n",
              "   'TO': 1,\n",
              "   'UH': 1,\n",
              "   'VB': 1,\n",
              "   'VBD': 11,\n",
              "   'VBG': 11,\n",
              "   'VBN': 1,\n",
              "   'VBP': 21,\n",
              "   'VBZ': 11,\n",
              "   'WDT': 1,\n",
              "   'WP': 1,\n",
              "   'WP$': 1,\n",
              "   'WRB': 1}},\n",
              " {'A': 136, 'B': 136})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vi) Using the adjusted frequencies, I recalculated the conditional probabilities to reflect the smoothing adjustments.\n"
      ],
      "metadata": {
        "id": "wwY1tY1wccVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the condprobs function to the smoothed frequencies\n",
        "smoothed_conditional_probabilities = condprobs(smoothed_classtagcounts, updated_classcounts)\n",
        "smoothed_conditional_probabilities"
      ],
      "metadata": {
        "id": "iJ6Gbwgqm6sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b449ba-69bc-4ff6-ceb0-5ae9861d7e2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': {'CC': 0.007352941176470588,\n",
              "  'CD': 0.007352941176470588,\n",
              "  'DT': 0.007352941176470588,\n",
              "  'EX': 0.007352941176470588,\n",
              "  'FW': 0.007352941176470588,\n",
              "  'IN': 0.007352941176470588,\n",
              "  'JJ': 0.22794117647058823,\n",
              "  'JJR': 0.007352941176470588,\n",
              "  'JJS': 0.007352941176470588,\n",
              "  'LS': 0.007352941176470588,\n",
              "  'MD': 0.007352941176470588,\n",
              "  'NN': 0.08088235294117647,\n",
              "  'NNS': 0.08088235294117647,\n",
              "  'NP': 0.007352941176470588,\n",
              "  'NPS': 0.007352941176470588,\n",
              "  'PDT': 0.007352941176470588,\n",
              "  'POS': 0.007352941176470588,\n",
              "  'PP': 0.007352941176470588,\n",
              "  'PP$': 0.007352941176470588,\n",
              "  'RB': 0.22794117647058823,\n",
              "  'RBR': 0.007352941176470588,\n",
              "  'RBS': 0.007352941176470588,\n",
              "  'RP': 0.007352941176470588,\n",
              "  'SYM': 0.007352941176470588,\n",
              "  'TO': 0.007352941176470588,\n",
              "  'UH': 0.007352941176470588,\n",
              "  'VB': 0.08088235294117647,\n",
              "  'VBD': 0.08088235294117647,\n",
              "  'VBG': 0.007352941176470588,\n",
              "  'VBN': 0.007352941176470588,\n",
              "  'VBP': 0.007352941176470588,\n",
              "  'VBZ': 0.007352941176470588,\n",
              "  'WDT': 0.007352941176470588,\n",
              "  'WP': 0.007352941176470588,\n",
              "  'WP$': 0.007352941176470588,\n",
              "  'WRB': 0.007352941176470588},\n",
              " 'B': {'CC': 0.007352941176470588,\n",
              "  'CD': 0.007352941176470588,\n",
              "  'DT': 0.007352941176470588,\n",
              "  'EX': 0.007352941176470588,\n",
              "  'FW': 0.007352941176470588,\n",
              "  'IN': 0.007352941176470588,\n",
              "  'JJ': 0.007352941176470588,\n",
              "  'JJR': 0.007352941176470588,\n",
              "  'JJS': 0.007352941176470588,\n",
              "  'LS': 0.007352941176470588,\n",
              "  'MD': 0.007352941176470588,\n",
              "  'NN': 0.15441176470588236,\n",
              "  'NNS': 0.22794117647058823,\n",
              "  'NP': 0.007352941176470588,\n",
              "  'NPS': 0.007352941176470588,\n",
              "  'PDT': 0.007352941176470588,\n",
              "  'POS': 0.007352941176470588,\n",
              "  'PP': 0.007352941176470588,\n",
              "  'PP$': 0.007352941176470588,\n",
              "  'RB': 0.007352941176470588,\n",
              "  'RBR': 0.007352941176470588,\n",
              "  'RBS': 0.007352941176470588,\n",
              "  'RP': 0.007352941176470588,\n",
              "  'SYM': 0.007352941176470588,\n",
              "  'TO': 0.007352941176470588,\n",
              "  'UH': 0.007352941176470588,\n",
              "  'VB': 0.007352941176470588,\n",
              "  'VBD': 0.08088235294117647,\n",
              "  'VBG': 0.08088235294117647,\n",
              "  'VBN': 0.007352941176470588,\n",
              "  'VBP': 0.15441176470588236,\n",
              "  'VBZ': 0.08088235294117647,\n",
              "  'WDT': 0.007352941176470588,\n",
              "  'WP': 0.007352941176470588,\n",
              "  'WP$': 0.007352941176470588,\n",
              "  'WRB': 0.007352941176470588}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vii) Finally, I used the prior probabilities and the smoothed conditional probabilities to compute the likelihood of each sentence belonging to classes A and B. I then predicted the most likely class for each sentence by selecting the class with the highest posterior probability, displaying each original sentence alongside its predicted classification.\n",
        "\n",
        "Through these steps, this project not only applies fundamental natural language processing techniques but also provides a systematic approach to text classification, leveraging the simplicity of Naive Bayes in the context of linguistic features.\n"
      ],
      "metadata": {
        "id": "DsA7z6NOcmHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_class(sentence, classpriors, conditional_probabilities, taglist):\n",
        "    # Splitting the sentence into words and POS tagging\n",
        "    words = sentence.split()\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Calculating probabilities for each class\n",
        "    class_scores = {}\n",
        "    for class_label in classpriors.keys():\n",
        "        class_scores[class_label] = classpriors[class_label]\n",
        "        for word, tag in tagged_words:\n",
        "            if tag in taglist:\n",
        "                class_scores[class_label] *= conditional_probabilities[class_label].get(tag, 1/len(taglist))\n",
        "\n",
        "    # Predicting the class with the highest probability\n",
        "    predicted_class = max(class_scores, key=class_scores.get)\n",
        "    return predicted_class\n",
        "\n",
        "# The sentences and other variables (classpriors, smoothed_conditional_probabilities, taglist) should be defined as per previous steps\n",
        "\n",
        "# Predicting the most likely class for each sentence\n",
        "predictions = [predict_class(sentence, classpriors, smoothed_conditional_probabilities, taglist) for sentence in sentences]\n",
        "\n",
        "# Printing out each original sentence alongside the prediction\n",
        "for sentence, prediction in zip(sentences, predictions):\n",
        "    print(f\"Sentence: \\\"{sentence}\\\" \\nPredicted Class: {prediction}\\n\")\n"
      ],
      "metadata": {
        "id": "y-g--614nQTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e616c1c-35c7-41dc-e32f-6791e13b8a28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"a tediously verbose sentence may contain many gratuitous and overly contrived modifiers .\" \n",
            "Predicted Class: A\n",
            "\n",
            "Sentence: \"another sentence could be too short .\" \n",
            "Predicted Class: A\n",
            "\n",
            "Sentence: \"some people write sentences that contain nouns and verbs , avoiding adjectives and descriptions .\" \n",
            "Predicted Class: B\n",
            "\n"
          ]
        }
      ]
    }
  ]
}